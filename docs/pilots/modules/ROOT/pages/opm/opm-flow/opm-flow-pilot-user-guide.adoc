# OPM Flow user guide for the MSO4SC portal

This user guide is intended to show how to use the OPM Flow application through the MSO4SC portal, step by step. As a prerequisite, you must have an account on a HPC or cloud system provider linked to the portal. At the moment, the CESGA HPC system Finis Terrae II ("ft2") is available, as well as some smaller clusters belonging to the universities of Strasbourg and Gy√∂r. More systems will be available in the future.

### 1. Log in to the MSO4SC portal.

The portal is found at https://portal.mso4sc.eu. In order to log in, you must first have signed up to be a user of the portal. After logging in you should see a screen with some big buttons for getting help and documentation, and a top menu for the most important actions of the portal.

### 2. Obtain the OPM Flow application.

In the top menu of the Portal, click on "Marketplace" to get the applications you want. In the marketplace, there are both free and non-free applications. OPM Flow is open source, and you do not have to pay anything for it in the marketplace. You still have to choose the application and "purchase" it though.

### 3. Look at the open testing datasets.

In the top menu, click on "Data Catalogue" to look at the available datasets. Here you can search for data sets, but the simplest way to look at those that are suited for running with Flow, is to click on "Groups" in the top menu of the component (it will be below the portal's top menu), and then on the group "OPM Flow datasets". At the moment, only 3 datasets are made openly available through the data catalogue. The smallest and quickest is the "SPE 1" dataset. You do not need to do anything about it now, you will choose that data set later in the process from within the experiments tool.

### 4. Enter and set up the Experiments tool.

In the top menu, click on "Experiments" to enter the Experiments tool. This is where you will control the setup and running of your application. In order to start jobs on your chosen HPC or cloud provider, you must first go to "Settings", found on the extreme right of the top menu of the tool (found just below the portal top menu). There you can enter your credentials for the computational provider, and give it a name of your choosing. For example, for the "ft2" system at CESGA, you must enter "ft2.cesga.es" in the "Host" field, as well as your username and password in the appropriate fields.

### 5. Deploy an experiment

Each experiment is an "instance" of the application. Every time you run an experiment using OPM Flow, you will interact with a new instance. At the moment, there are five steps: create, deploy, run, undeploy and destroy. It is likely that this will be simplified in the future. To start, click "Create App Instance". From the "Application" menu that appears, choose "OPM Flow". Then automatically, a form will appear that allows you to customize your run. We will go through all items in order, the ones marked as bold should be edited or checked:

 - **Deployment ID:** This is the name of the instance you are creating. It should be sufficiently unique for you to tell it apart from other instances you may want to create. For your first run, you could call it "First test run of OPM Flow" for example.
 - **Application:** This is the application you have chosen. Here we will describe the "OPM Flow" application.
 - **HPC: primary:** This is the computational provider you want to use. Choose the one you set up in step 4 above.
 - HPC: secondary: This is not used by OPM Flow, leave it at "None".
 - **Dataset resource: input_url:** This lets you choose one of the open datasets from the Data Catalogue to run. Choose "spe-1" for your first test. When you choose a data set, a list of available files will appear. For "spe-1" the list contains just a single file, called "spe1.tgz". Choose that by clicking the round selection button to the left of the file name.
 - base_dir: This is the directory on the computational system where all files will be put, both input and output. It defaults to "$LUSTRE" which is appropriate for HPC systems running Lustre such as "ft2", but you may need to change this for your system.
 - **hpc_partition:** The computational partition on which to launch the job. For "ft2" it is often appropriate to run on "thin-shared".
 - **hpc_reservation:** If your user has a special reservation on the computational system you can indicate that here. Otherwise it should be left blank.
 - job_prefix: This is arbitrary, and can be used to easily indicate which jobs are started from the portal.
 - **max_time:** This is the maximum time that will be used for your job, in HH:MM:SS format. For the "spe-1" case you can choose one minute, it should run in a few seconds. For the "norne" case you may want to request 30 minutes if running on just one core, or less if running in parallel (see below).
 - monitor_entrypoint: Leave blank.
 - parallel_nodes: The number of computational nodes on which to run Flow. Leave at 1 for now.
 - parallel_tasks: The number of parallel tasks or processes used. For your initial test leave at 1. Later, you may use this to request a higher number of cores from the node, up to 24 for the "ft2" systems.
 - parallel_tasks_per_node: Set this equal to parallel_tasks for now.
 - singularity_image_filename: The Singularity image containing the application will be saved on the computational system using this name.
 - singularity_image_uri: The Singularity image will be obtained from this URI. Leave it at the default unless you need to use an older release for example.

After customizing your run, click the "Create Instance!" button. You should get a confirmation saying that your intance was created.

Now click on "Deploy Instance", choose your newly made instance from the "App Instance" drop down menu, and click "Deploy". You may occasionally see a message that deployment is not ready yet, if you see that message you can wait a few seconds and click Deploy again. You should now see a log with lots of messages such as "Configuring node", "Starting node" etc. These are mostly for developers to debug the application, so you can ignore them. While deploying, there will be a blue indicator next to the "Deployment" dropdown, it will turn green when deployment is complete, or red if there is an error. The deployment stage obtains the application Singularity image and the input data set, storing both on the computational system.

### 6. Run an experiment.

Click "Run Instance" in the Experiment tool top menu. Choose the newly created and deployed instance, and click "Run!". The indicator button will turn blue while running, and green when complete. If there is an error, it will turn red. The log messages are (again) mostly for the developers to debug the system (they are messages from the Cloudify system), but it may be useful to see the state changes there. When initiated, a run will be PENDING, while we wait for an available time slot on the computational system. This stage may last for hours or days if you requested large resources (time and parallel tasks) on a contested system, or less than a second if you have dedicated resources or a small resource request. When it actually starts, it will be RUNNING, and finally COMPLETED when done.

You should then clean up the instance by clicking "Undeploy/Clean Up Instance" in the menu, choosing your instance, and clicking the "Undeploy!" button. This will remove the input data and application Singularity image, but leave your simulation output in place. Finally, click "Destroy App Instance" in the menu, choose your instance, and click the "Destroy Instance!" button.

### 7. Look at the results.

Your output is now stored on the computational system. At the moment, there is no extra support to download the output data, you will have to access it as you normally would access data on that system, by using scp or rclone for example. However if your computational provider offers visualization support by noVNC, you can access that by clicking "Visualization" in the Portal top menu (leaving the Experiment tool). Using the Settings there you can set up a remote desktop and visualize results using tools installed on that system. For "ft2", you can use the following settings:
 - host: vis.lan.cesga.es
 - list command: /opt/cesga/vis/bin/desktops
 - create commmand: /opt/cesga/vis/bin/getdesktop

After adding your settings, you can click "Desktops Available", choose the system and create a new desktop. If using "ft2" you can visualize results in ResInsight, but it requires two steps in the terminal:
 - Make ResInsight available by "module load resinsight"
 - Run it by "vglrun resinsight".
